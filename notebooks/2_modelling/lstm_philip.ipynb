{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = pd.read_parquet(\"../../data/preprocessed/holdout_1.pq\") # , engine='pyarrow'\n",
    "train_y = pd.read_parquet(\"../../data/preprocessed/holdout_1_y.pq\")\n",
    "test_x = pd.read_parquet(\"../../data/preprocessed/thursdays.pq\")\n",
    "test_y = pd.read_parquet(\"../../data/preprocessed/thursdays_y.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "478807"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 0.0244, MAE: 0.1211\n",
      "0.967391304347826\n",
      "Epoch 2/30, Loss: 0.0219, MAE: 0.0601\n",
      "0.967391304347826\n",
      "Epoch 3/30, Loss: 0.0214, MAE: 0.0733\n",
      "0.9510869565217391\n",
      "Epoch 4/30, Loss: 0.0211, MAE: 0.0960\n",
      "0.9782608695652174\n",
      "Epoch 5/30, Loss: 0.0209, MAE: 0.1094\n",
      "0.9782608695652174\n",
      "Epoch 6/30, Loss: 0.0209, MAE: 0.1170\n",
      "0.9891304347826086\n",
      "Epoch 7/30, Loss: 0.0208, MAE: 0.1204\n",
      "0.9510869565217391\n",
      "Epoch 8/30, Loss: 0.0208, MAE: 0.1232\n",
      "0.9891304347826086\n",
      "Epoch 9/30, Loss: 0.0207, MAE: 0.1234\n",
      "0.9782608695652174\n",
      "Epoch 10/30, Loss: 0.0206, MAE: 0.1251\n",
      "0.9782608695652174\n",
      "Epoch 11/30, Loss: 0.0207, MAE: 0.1268\n",
      "0.907608695652174\n",
      "Epoch 12/30, Loss: 0.0206, MAE: 0.1273\n",
      "0.9782608695652174\n",
      "Epoch 13/30, Loss: 0.0206, MAE: 0.1279\n",
      "0.9293478260869565\n",
      "Epoch 14/30, Loss: 0.0206, MAE: 0.1289\n",
      "0.967391304347826\n",
      "Epoch 15/30, Loss: 0.0206, MAE: 0.1298\n",
      "0.9891304347826086\n",
      "Epoch 16/30, Loss: 0.0206, MAE: 0.1296\n",
      "0.9239130434782609\n",
      "Epoch 17/30, Loss: 0.0206, MAE: 0.1298\n",
      "0.9619565217391305\n",
      "Epoch 18/30, Loss: 0.0206, MAE: 0.1300\n",
      "0.9456521739130435\n",
      "Epoch 19/30, Loss: 0.0205, MAE: 0.1296\n",
      "0.9782608695652174\n",
      "Epoch 20/30, Loss: 0.0205, MAE: 0.1305\n",
      "0.9782608695652174\n",
      "Epoch 21/30, Loss: 0.0205, MAE: 0.1303\n",
      "0.9782608695652174\n",
      "Epoch 22/30, Loss: 0.0205, MAE: 0.1309\n",
      "0.9293478260869565\n",
      "Epoch 23/30, Loss: 0.0205, MAE: 0.1310\n",
      "0.967391304347826\n",
      "Epoch 24/30, Loss: 0.0205, MAE: 0.1320\n",
      "0.9619565217391305\n",
      "Epoch 25/30, Loss: 0.0205, MAE: 0.1319\n",
      "0.9456521739130435\n",
      "Epoch 26/30, Loss: 0.0205, MAE: 0.1321\n",
      "0.9565217391304348\n",
      "Epoch 27/30, Loss: 0.0205, MAE: 0.1316\n",
      "0.9891304347826086\n",
      "Epoch 28/30, Loss: 0.0205, MAE: 0.1311\n",
      "1.0\n",
      "Epoch 29/30, Loss: 0.0204, MAE: 0.1315\n",
      "0.967391304347826\n",
      "Epoch 30/30, Loss: 0.0204, MAE: 0.1313\n",
      "0.9619565217391305\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "target_column = 'ProzessData_ActData_AB1_Temperature_DR1_MassMixingStage'\n",
    "\n",
    "# Normalize features in train_x (excluding the timestamp column)\n",
    "scaler_x = MinMaxScaler()\n",
    "train_x_scaled = scaler_x.fit_transform(train_x.iloc[:, 1:])  # Adjust if the first column isn't the timestamp\n",
    "\n",
    "# Normalize the target column in train_y\n",
    "scaler_y = MinMaxScaler()\n",
    "train_y_scaled = scaler_y.fit_transform(train_y[[target_column]])\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_x_tensor = torch.tensor(train_x_scaled).float().unsqueeze(1)  # Add time step dimension\n",
    "train_y_tensor = torch.tensor(train_y_scaled).float()\n",
    "\n",
    "# DataLoader\n",
    "train_dataset = TensorDataset(train_x_tensor, train_y_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.dropout(lstm_out[:, -1, :])\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Model initialization\n",
    "input_dim = train_x_tensor.shape[2]  # Number of features\n",
    "hidden_dim = 50  # Example value\n",
    "output_dim = train_y_tensor.shape[1]  # Should be 1 as we're predicting a single target\n",
    "\n",
    "model = LSTMModel(input_dim, hidden_dim, output_dim).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    total_mae = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Calculate loss (MSE)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate MAE\n",
    "        #print(type(F.l1_loss(outputs, labels, reduction='sum').item()))\n",
    "        list1 = outputs.tolist()\n",
    "        list2 = labels.tolist()\n",
    "        list1 = np.array(list1).flatten()\n",
    "        list2 = np.array(list2).flatten()\n",
    "        abweichung = []\n",
    "        for a, b in zip(list1, list2):\n",
    "            \n",
    "            abweichung.append(abs(a - b))    # Give points\n",
    "        points = []\n",
    "        for diff in abweichung:\n",
    "            if diff < 0.05:\n",
    "                points.append(1.0)\n",
    "            elif 0.05 <= diff < 0.1:\n",
    "                points.append(0.5)\n",
    "            elif 0.1 <= diff < 0.5:\n",
    "                points.append(0.25)\n",
    "            else:\n",
    "                points.append(0) \n",
    "\n",
    "        list1 = points\n",
    "\n",
    "        points2 = [] \n",
    "        consecutive_count =0      \n",
    "        for i in range(0, len(list1)):\n",
    "            \n",
    "            if list1[i] >=0.5 and consecutive_count == 0 :\n",
    "\n",
    "                points2.append (1)\n",
    "\n",
    "            elif list1[i] >=0.5 and consecutive_count != 0:\n",
    "\n",
    "                if consecutive_count == 1:\n",
    "                    points2.append (.5)\n",
    "                    points2.append(1)\n",
    "        \n",
    "                elif 2 <= consecutive_count <= 10:\n",
    "                    points2.extend([0.25] * consecutive_count)\n",
    "                    points2.append(1)\n",
    "                else:\n",
    "                    points2.extend([0] * consecutive_count)\n",
    "                    points2.append(1)\n",
    "\n",
    "                consecutive_count = 0# Reset consecutive count\n",
    "\n",
    "            else:\n",
    "                consecutive_count+=1\n",
    "\n",
    "        \n",
    "        if consecutive_count == 1:\n",
    "            points2.append (.5)\n",
    "        elif 2 <= consecutive_count <= 0.01:\n",
    "            points2.extend([0.25] * consecutive_count)\n",
    "        else:\n",
    "            points2.extend([0] * consecutive_count)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        total = sum(points)+sum(points2) \n",
    "        maxpoint = len(points)*2\n",
    "\n",
    "        #print(\" Punkte aus abweichungslist:\", sum(points), \"Punkte aus Zeitabweichung:\", sum(points2), \"max Punkte:\",len(points))\n",
    "\n",
    "        #print(total/maxpoint)\n",
    "        \n",
    "        mae = ((maxpoint/total)-1)*100\n",
    "        total_mae += mae\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Average loss and MAE over the epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_mae = total_mae / len(train_loader.dataset)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, MAE: {avg_mae:.4f}')\n",
    "    print(total/maxpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming your model is named 'model'\n",
    "torch.save(model, '../../models/lstm_philip2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/OWL_Factory_Hackathon/notebooks/2_modelling/lstm_philip.ipynb Zelle 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bfictional-lamp-g66rv647gwvfpw6/workspaces/OWL_Factory_Hackathon/notebooks/2_modelling/lstm_philip.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m../../models/lstm_philip.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m----> <a href='vscode-notebook-cell://codespaces%2Bfictional-lamp-g66rv647gwvfpw6/workspaces/OWL_Factory_Hackathon/notebooks/2_modelling/lstm_philip.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     pickle\u001b[39m.\u001b[39mdump(model, file)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "with open('../../models/lstm_philip2.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jhn_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "7ba48d1dfc31994510d1202d5cc1ad7c5341fe2fe772e21645b10ef4af5c8f4c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
